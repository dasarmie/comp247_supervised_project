# -*- coding: utf-8 -*-
"""fatal_collisions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/ujjwal-poudel/Fatal-Collisions-Predictions/blob/main/fatal_collisions.ipynb
"""

# from google.colab import files


# uploaded = files.upload()

# import pandas as pd
# import io

import pandas as pd
file_path = '/Users/ujjwalpoudel/Documents/projects/Fatal-Collisions-Predictions/src/Killed_and_Seriously_Injured.csv'

df_accidents = pd.read_csv(file_path)

# df_accidents = pd.read_csv(io.BytesIO(uploaded['Killed_and_Seriously_Injured (2).csv']))
# print(df_accidents)

print(df_accidents)

'''
Imports
'''
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

"""Data preprocessing"""

# Loading the data
print(df_accidents.head(3))

#explore numerical features
print('\nDescriptions')
print(df_accidents.describe())

#shape of data
print('\nShape')
print(df_accidents.shape)

#explore datatypes
print('\nData types')
print(df_accidents.dtypes)

#get column list and sort
print('\nColums value soreted')
print(sorted(df_accidents.columns.values))
# Check for missing values

print('\nData info')
print(df_accidents.isnull().sum())
print(df_accidents.info())

# Get column names
column_names = df_accidents.columns

# Convert to a list if needed
column_names_list = list(column_names)

print(column_names)
print(column_names_list)

# #identify non-numeric columns
# non_numeric_cols = df_accidents.select_dtypes(exclude=['number']).columns
# print(f"Non-numeric columns:\t {non_numeric_cols}")

#target column
print(df_accidents["INJURY"].value_counts())

#drop rows containing null values
df_cleaned = df_accidents.dropna(subset=['INJURY'])

# Create new binary class based on column INJURY
df_accidents["is_fatal"] = np.where(df_accidents["INJURY"] == "Fatal", 1, 0)

# Fill missing values with mode (most frequent value)
# First, calculate the mode for each column
modes = df_accidents.mode().iloc[0]

# Then, fill missing values with the calculated mode
df_accidents.fillna(modes, inplace=True)

# Verify 'is_fatal' column
print("\nis_fatal column value counts:")
print(df_accidents["is_fatal"].value_counts())

# Visualize relationships
sns.countplot(x='DISTRICT', hue='is_fatal', data=df_accidents)
plt.show()
#plot histogram
df_accidents.hist(bins=50, figsize=(20, 15))
plt.show()

df_accidents.drop(columns=['INDEX_','ACCNUM','OBJECTID','INVAGE','OFFSET','INITDIR','VEHTYPE','MANOEUVER',
                           'DRIVACT', 'DRIVCOND', 'PEDTYPE', 'PEDACT', 'PEDCOND', 'CYCLISTYPE', 'CYCACT',
                           'CYCCOND', 'PEDESTRIAN', 'CYCLIST', 'DIVISION','ACCLASS'],
                   inplace=True)

# Identifing numeric and non-numeric columns
numeric_cols = df_accidents.select_dtypes(include=['number']).columns
non_numeric_cols = df_accidents.select_dtypes(exclude=['number']).columns
print(numeric_cols)
print(non_numeric_cols)

# Defining target variable and features
y = df_accidents['is_fatal']
X = df_accidents.drop('is_fatal', axis=1)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=84)

# Defining preprocessing for numeric and categorical features
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combining preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, non_numeric_cols)
    ])

# Defining models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(),
    'SVM': SVC()
}

# Creating pipelines for each model with SMOTE
pipelines = {name: ImbPipeline(steps=[('preprocessor', preprocessor), ('smote', SMOTE()), ('classifier', model)])
             for name, model in models.items()}

# Fine tuning the models using Grid Search
'''
Logistic Regression
'''
param_grid_lr = {
    'classifier__C': [0.1, 1, 10, 100],
    'classifier__solver': ['lbfgs', 'liblinear']
}

'''
Decision Tree
'''
param_grid_dt = {
    'classifier__max_depth': [3, 5, 7, 10],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4]
}

'''
SVM
'''
param_grid_svc = {
    'classifier__C': [0.1, 1, 10, 100],
    'classifier__kernel': ['linear', 'rbf', 'poly'],
    'classifier__gamma': ['scale', 'auto']
}

grid_searches = {
    'Logistic Regression': GridSearchCV(pipelines['Logistic Regression'], param_grid_lr, cv=5, n_jobs=-1),
    'Decision Tree': GridSearchCV(pipelines['Decision Tree'], param_grid_dt, cv=5, n_jobs=-1),
    'SVM': GridSearchCV(pipelines['SVM'], param_grid_svc, cv=5, n_jobs=-1)
}

# Performing Grid Search and printing best parameters
for name, grid_search in grid_searches.items():
    grid_search.fit(X_train, y_train)
    print(f"Best parameters for {name}: {grid_search.best_params_}")
    y_pred = grid_search.predict(X_test)
    print(f"Classification report for {name} after tuning:")
    print(classification_report(y_test, y_pred))